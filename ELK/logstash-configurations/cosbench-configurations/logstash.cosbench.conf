####################################################
#
# This logstash configuration will process COSBench workstage result CSV files
# These files are in the COSBench archive folder at: 
#   /opt/cosbench/cos/archive/<workload-name>/<stage-name>.csv
# 
# IMPORTANT Information about running this config: 
#   This config must be run single threaded because the CSV 
#   files MUST be processed sequentially from beginning to end!
#   To run single threaded you MUST use the -w 1 argument!
#   To re-process existing files you must rm sincedb file.
#
# To run this configuration:
#   export ELASTIC_IP=192.168.1.1
#   sudo ./logstash -w 1 -f /etc/logstash/manual_configs/logstash.cosbench.conf;
####################################################

input {
  file {
    path => "/opt/cosbench/cos/archive/run-history.csv"
    sincedb_path => "/etc/logstash/cosbench-run-history.sincedb"
    start_position => "beginning"
    stat_interval => 1
    type => "cosbench-run-history"
  }
  
  file {
    path => "/opt/cosbench/cos/archive/w*/s*.csv"
    sincedb_path => "/etc/logstash/cosbench-archive.sincedb"
    start_position => "beginning"
    stat_interval => 5
    type => "cosbench-workstage"
  }
  
#  stdin {
#    type => "cosbench-workstage"
#  }
}

filter {
  if [type] == "cosbench-run-history" {
    if [message] =~ "^Id,.*" { 
      #drop headers
      drop {}
    }
    csv {
        columns => ["Id", "workload_name", "Submitted-At", "Started-At", "Stopped-At", "Op-Info", "State", "Detailed State"]
    } 
 
    grok {
      break_on_match => false
      match => {
        "Id" => "w%{INT:workload_number:int}"
      }  
    }    
    
    date {
      match => [ "Started-At" , "yyyy-MM-dd HH:mm:ss"]
    } 
    date {
      match => [ "Started-At" , "yyyy-MM-dd HH:mm:ss"]
      target => "Started-At"
    }
    date {
      match => [ "Stopped-At" , "yyyy-MM-dd HH:mm:ss"]
      target => "Stopped-At"
    }
    
    mutate { 
      remove_field => ["Submitted-At"]
      remove_field => ["Op-Info"]
      remove_field => ["Detailed State"]
      remove_field => ["column9"]
      remove_field => ["column10"]
      remove_field => ["path"]
      remove_field => ["type"]
      remove_field => ["message"]
    }    
    
  } else {
    ruby{
      init => "
        @@startDay = nil
        @@lastTime = nil
        @@csv_type = ''
        @@lastWkLd = nil
      "
      code => "
        begin
          message = event.get('message')
          if /^,.*/.match(message)
            #Parse header row to determine the csv type
            puts ' Detecting format for file: ' + event.get('path')
            if       /^,read,read,read.*/.match(message) then @@csv_type = 'r'
            elsif /^,write,write,write.*/.match(message) then @@csv_type = 'w'
            elsif /^,write,read,delete.*/.match(message) then @@csv_type = 'wrd'
            else
              @@csv_type = 'unk'
              puts ' Unknown format for file: ' + event.get('path')
            end
            #puts ' set csv type to ' + @@csv_type      
          end

          event.set('csv_type', @@csv_type)
          # puts Thread.current.object_id.to_s + ' csv type is: ' + @@csv_type
          # puts Thread.current.object_id.to_s + ' message: ' + event.get('message')
        rescue => detail
          puts 'Fail Determining File Format \n'
          puts detail.inspect
          print detail.backtrace.join('\n')
        end        
      " 
    }

    if [message] =~ "^Timestamp,.*" or [message] =~ "^,.*" { 
      #drop headers
      drop {}
    }
    if "N/A" in [message] {
      #Drop N/A rows
      drop {}
    }


    grok {
      break_on_match => false
      match => {
        #"path" => "/opt/cosbench/cos/archive/w%{INT:workload_number:int}-.*/(?<stage_name>.+)\.csv"
        "path" => "/opt/cosbench/cos/archive/(?<workload_name>w.+)/(?<stage_name>s.+)\.csv"
      } 
      match => {
        "workload_name" => "w%{INT:workload_number:int}.*"
        "stage_name"    => "s%{INT:stage_number:int}.*"
      }     
    }


    mutate {  
      add_field => {
         "writes" => "0"
         "reads" => "0"
         "deletes" => "0"
         "write-bytes" => "0"
         "read-bytes" => "0"
         "delete-bytes" => "0"
         "write-latency" => "0.0"
         "read-latency" => "0.0"
         "delete-latency" => "0.0"
         "write-time" => "0.0"
         "read-time" => "0.0"
         "delete-time" => "0.0"
         "write-Ops" => "0.0"
         "read-Ops" => "0.0"
         "delete-Ops" => "0.0"
         "write-Bps" => "0.0"
         "read-Bps" => "0.0"
         "delete-Bps" => "0.0"
         "write-success" => "0.0"
         "read-success" => "0.0"
         "delete-success" => "0.0"
      }    
    }  

    if [csv_type] == "w" {
      csv {
          columns => ["Timestamp", "writes", "write-bytes", "write-latency", "write-time", "write-Ops", "write-Bps", "write-success", "Min-Version", "Version", "Max-Version"]
      }
    } else if [csv_type] == "r" {
      csv {
          columns => ["Timestamp", "reads", "read-bytes", "read-latency", "read-time", "read-Ops", "read-Bps", "read-success", "Min-Version", "Version", "Max-Version"]
      }
    } else if [csv_type] == "wrd" {
      csv {
          columns => ["Timestamp", "writes", "reads", "deletes", "write-bytes", "read-bytes", "delete-bytes", "write-latency", "read-latency", "delete-latency", "write-time", "read-time", "delete-time", "write-Ops", "read-Ops", "delete-Ops", "write-Bps", "read-Bps", "delete-Bps", "write-success", "read-success", "delete-success", "Min-Version", "Version", "Max-Version"]
      }
    } else {
      drop {}
    }

    mutate {  
      convert => {
         "writes" => "integer"
         "reads" => "integer"
         "deletes" => "integer"
         "write-bytes" => "integer"
         "read-bytes" => "integer"
         "delete-bytes" => "integer"
         "write-latency" => "float"
         "read-latency" => "float"
         "delete-latency" => "float"
         "write-time" => "float"
         "read-time" => "float"
         "delete-time" => "float"
         "write-Ops" => "float"
         "read-Ops" => "float"
         "delete-Ops" => "float"
         "write-Bps" => "float"
         "read-Bps" => "float"
         "delete-Bps" => "float"
         "write-success" => "float"
         "read-success" => "float"
         "delete-success" => "float"
      }    
    }

    ruby{
      code => "
        begin
          currentWkLd = event.get('workload_number')
          if @@lastWkLd != currentWkLd
            event.set('start_day', '')
            @@startDay = nil
            puts 'Processing new workload number: ' + currentWkLd.to_s
          end
          @@lastWkLd = currentWkLd
        rescue => detail
          puts 'Fail Initializing Start Date for new workload \n'
          puts detail.inspect
          print detail.backtrace.join('\n')
        end    
      "
    }

    if [start_day] == "" {
      elasticsearch {
        hosts => ["${ELASTIC_IP}:9200"]
        user => "elastic"
        password => "changeme"
        index => "cosbench-run-history"
        query => "workload_number:%{[workload_number]}"
        fields => { "@timestamp" => "start_day" }
      }
    }    

    ruby{
      code => "
        begin
          if @@startDay.nil?
            @@startDay = DateTime.parse(event.get('start_day')).to_time.localtime.to_datetime
            puts 'Workload start time is: ' + @@startDay.strftime('%c') + ' ' + @@startDay.strftime('%z')
          end
          full_date_time = @@startDay.strftime('%F') + ' ' + event.get('Timestamp') + ' ' + DateTime.now.strftime('%z')
          if @@lastTime and DateTime.parse(@@lastTime) > DateTime.parse(full_date_time)
            puts 'Current time is less than last time!'
            puts 'Current time:  ' + full_date_time
            puts 'Last time:     ' + @@lastTime
            @@startDay = @@startDay + 1
            full_date_time = @@startDay.strftime('%F') + ' ' + event.get('Timestamp') + ' ' + DateTime.now.strftime('%z')
            puts 'New current:   ' + full_date_time
          end
          event.set('full_date_time', full_date_time)
          @@lastTime = full_date_time
        rescue => detail
          puts 'Failure computing full date and time'
          puts detail.inspect
          print detail.backtrace.join('\n')
        end    
      "
    }

    date {
      match => [ "full_date_time" , "yyyy-MM-dd HH:mm:ss Z"]
    } 

    mutate { 
      add_field => {
         "workload_workstage" => "w%{workload_number}_s%{stage_number}"
      } 
      remove_field => [csv_type]
      remove_field => [message]
      remove_field => [full_date_time]
      remove_field => [host]
      remove_field => [Timestamp]
      remove_field => [path]
      remove_field => [type]
      remove_field => ["Max-Version"]
      remove_field => ["Min-Version"]
      remove_field => ["Version"]
      remove_field => ["start_day"]
    }
  }
}

output {
  if [Started-At] {
    elasticsearch {
      hosts => ["${ELASTIC_IP}:9200"]
      user => "elastic"
      password => "changeme"
      index => "cosbench-run-history"
    }
  } else {
    elasticsearch {
      hosts => ["${ELASTIC_IP}:9200"]
      user => "elastic"
      password => "changeme"
      index => "cosbench-stage-%{+yyMMdd}"
    }
  }
#  stdout { 
#    codec => rubydebug 
#  }
}
